{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions (Jupyter Notebooks and Self Driving Cars Udacity Nanodegree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in an image\n",
    "image = mpimg.imread('test_images/solidWhiteRight.jpg')\n",
    "\n",
    "#printing out some stats and plotting\n",
    "print('This image is:', type(image), 'with dimensions:', image.shape)\n",
    "plt.imshow(image)  # if you wanted to show a single color channel image called 'gray', \n",
    "# for example, call as plt.imshow(gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filenames = os.listdir(\"test_images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    (assuming your grayscaled image is called 'gray')\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
    "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1., γ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + γ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, γ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_output = 'test_videos_output/solidWhiteRight.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the video inline, or if you prefer find the video in your filesystem (should be in the same directory) and play it in your video player of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Display Side by Side images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image', fontsize=50)\n",
    "ax2.imshow(grad_binary, cmap='gray')\n",
    "ax2.set_title('Thresholded Gradient', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Camera Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now based on al objpoints + imgpoints, will compute camera calibration matrix \n",
    "# and distortion coefficients (mtx and dist)\n",
    "#ret, mtx, dist, rvecs, tvers = cv2.calibrateCamera(objpoints,imgpoints,gray.shape[::-1],None,None)\n",
    "ret, mtx, dist, rvecs, tvers = cv2.calibrateCamera(objpoints,imgpoints,(xSize,ySize),None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cameraCalibration() :\n",
    "    # images for camera calibration are stored in the folder called `camera_cal`.  \n",
    "    # need to set your chessboard size to 9x6 for the project instead of 8x6 as in the lesson.\n",
    "\n",
    "    # multiple pictures of the chessboard against a flat surface, \n",
    "    # then we’ll be able to detect any distortions by  looking at the difference between apparent \n",
    "    # size and the shape of squares in these images, and the size and shape that they actually are.\n",
    "    #    • Then we’ll use that information to calibrate our camera.\n",
    "    #    • Create a transform that maps distorted point to undistorded points.\n",
    "    #    • And finally, undistort any images.\n",
    "    \n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "\n",
    "    # Make a list of calibration images\n",
    "    #images = glob.glob('camera_cal/calibration*.jpg')\n",
    "    filenames = os.listdir(cameraCalFolder)\n",
    "\n",
    "    # Step through the list and search for chessboard corners\n",
    "    for fname in filenames :\n",
    "        img = mpimg.imread(cameraCalFolder + '/' + fname)\n",
    "\n",
    "        ret, corners, objpoints, imgpoints = findChessboardCorners(img, objpoints, imgpoints)\n",
    "\n",
    "    # Now based on al objpoints + imgpoints, will compute camera calibration matrix \n",
    "    # and distortion coefficients (mtx and dist)\n",
    "    ret, mtx, dist, rvecs, tvers = cv2.calibrateCamera(objpoints,imgpoints,(xSize,ySize),None,None)  \n",
    "    \n",
    "    return mtx, dist\n",
    "    # end cameraCalibration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes an image, object points, and image points\n",
    "# performs the camera calibration, image distortion correction and \n",
    "# returns the undistorted image\n",
    "def cal_undistort(img, objpoints, imgpoints):\n",
    "    # Use cv2.calibrateCamera() and cv2.undistort()\n",
    "    ret, mtx, dist, rvecs, tvers = cv2.calibrateCamera(objpoints,imgpoints,img.shape[::-1],None,None)\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist\n",
    "\n",
    "def display2ImagesSideBySide(img1,txt1,img2,txt2) :\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title(txt1, fontsize=50)\n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title(txt2, fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "def saveImageOutputAndShowVsImageInput(saveFolder, filenames_in, fname, img_in, legend_in,\n",
    "                                       img_out, legend_out,saveGray=False) :\n",
    "    # save and show output images -- for Writeup illustrations\n",
    "    if saveImages :\n",
    "        if not saveGray :\n",
    "            mpimg.imsave( saveFolder + '/' + fname,img_out)\n",
    "        else : \n",
    "            mpimg.imsave( saveFolder + '/' + fname,img_out,cmap='gray')\n",
    "    # display exemple for first chessboard image\n",
    "    if fname == filenames_in[0] :\n",
    "        display2ImagesSideBySide(img_in,legend_in,img_out,legend_out)\n",
    "        \n",
    "def saveImageOutputAndShow(saveFolder, filenames_in, fname, img_out,saveGray=False) :\n",
    "    # save and show output images -- for Writeup illustrations\n",
    "    if saveImages :\n",
    "        if not saveGray :\n",
    "            mpimg.imsave( saveFolder + '/' + fname,img_out)\n",
    "        else : \n",
    "            mpimg.imsave( saveFolder + '/' + fname,img_out,cmap='gray')\n",
    "    # display exemple for first chessboard image\n",
    "    if fname == filenames_in[0] :\n",
    "        plt.imshow(img_out)\n",
    "    \n",
    "\n",
    "def getSizeImages() : \n",
    "    # Make a list of Thresholded Binary Images\n",
    "    filenames = os.listdir(testImageFolder)\n",
    "    img = mpimg.imread(testImageFolder + '/' + filenames[0])\n",
    "    return img.shape[1], img.shape[0]\n",
    "\n",
    "\n",
    "# keep track of things like where your last several detections of the lane lines \n",
    "# were and what the curvature was, so you can properly treat new detections. \n",
    "# To do this, it's useful to define a Line() class to keep track of all the \n",
    "# interesting parameters you measure from frame to frame.\n",
    "# Define a class to receive the characteristics of each line detection\n",
    "\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        # set all members to their initial value\n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        #x values for detected line pixels\n",
    "        self.allx = None  \n",
    "        #y values for detected line pixels\n",
    "        self.ally = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPerspectiveTransformMatrix():\n",
    "        \n",
    "    #define 4 source points src = np.float32([[,],[,],[,],[,]])\n",
    "    src=np.float32([[265,678],[1042,678],[582,460],[702,460]])\n",
    "    #src=np.float32([[253,678],[1054,678],[608,441],[672,441]])\n",
    "    #define 4 destination points src = np.float32([[,],[,],[,],[,]])\n",
    "    dst=np.float32([[265,ySize-1],[1042,ySize-1],[265,0],[1042,0]])\n",
    "    #dst=np.float32([[253,ySize-1],[1054,ySize-1],[253,0],[1054,0]])\n",
    "\n",
    "    # use cv2.getPerspectiveTransform() to get M, the transform matrix\n",
    "    M = cv2.getPerspectiveTransform(src,dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst,src)\n",
    "    \n",
    "    return M, Minv\n",
    "\n",
    "# Those variables will be used later on as well --> extracted from loops\n",
    "M, Minv = getPerspectiveTransformMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chessboard corner detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images for camera calibration are stored in the folder called `camera_cal`.  \n",
    "# need to set your chessboard size to 9x6 for the project instead of 8x6 as in the lesson.\n",
    "\n",
    "# multiple pictures of the chessboard against a flat surface, \n",
    "# then we’ll be able to detect any distortions by  looking at the difference between apparent \n",
    "# size and the shape of squares in these images, and the size and shape that they actually are.\n",
    "#    • Then we’ll use that information to calibrate our camera.\n",
    "#    • Create a transform that maps distorted point to undistorded points.\n",
    "#    • And finally, undistort any images.\n",
    "\n",
    "def findChessboardCorners(img, objpoints, imgpoints) :\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((6*9,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "    # convert image to gray scale, \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "         \n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (9,6),None)\n",
    "\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "        \n",
    "    return ret, corners, objpoints, imgpoints\n",
    "\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warpImage(img):\n",
    "    \n",
    "    img_size = (xSize,ySize)\n",
    "    warped = cv2.warpPerspective(img,M,img_size,flags=cv2.INTER_LINEAR)\n",
    "    return warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePolynomial2nddegree(coeff,y) : \n",
    "    x = ((coeff[0]*(y**2)) + coeff[1]*y + coeff[2]).astype(int)\n",
    "    return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCurveDeviationOverlay(laneBoundaryImg, left_fit, right_fit, ploty,left_curverad, right_curverad, deviation) :\n",
    "    \n",
    "    curv_avg = int((left_curverad+right_curverad)/2)\n",
    "    \n",
    "    # print radius in overlay on boundaryLaneImage ...\n",
    "    # Radius of curvature = xxxx(m)\n",
    "    # Vehicule is x.xxm left/right of center\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    if deviation <= 0 :\n",
    "        side = 'left'\n",
    "    else : \n",
    "        side = 'right'\n",
    "    txt1 = f'Radius of Curvature = {curv_avg}(m)'\n",
    "    txt2 = f'Vehicle is {np.absolute(deviation):.2f}m {side} of center'\n",
    "    \n",
    "    # cv2.putText(image, text, org, font, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]])\n",
    "    cv2.putText(laneBoundaryImg, txt1, (50,50), font, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(laneBoundaryImg, txt2, (50,100), font, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    return laneBoundaryImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    a = 0.1\n",
    "    b = 0.9\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + ( ( (image_data - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, file):\n",
    "    \"\"\"\n",
    "    Download file from <url>\n",
    "    :param url: URL to file\n",
    "    :param file: Local file path\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        print('Downloading ' + file + '...')\n",
    "        urlretrieve(url, file)\n",
    "        print('Download Finished')\n",
    "\n",
    "# Download the training and test dataset.\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'notMNIST_train.zip')\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'notMNIST_test.zip')\n",
    "\n",
    "# Make sure the files aren't corrupted\n",
    "assert hashlib.md5(open('notMNIST_train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa',\\\n",
    "        'notMNIST_train.zip file is corrupted.  Remove the file and try again.'\n",
    "assert hashlib.md5(open('notMNIST_test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9',\\\n",
    "        'notMNIST_test.zip file is corrupted.  Remove the file and try again.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    \"\"\"\n",
    "    Uncompress features and labels from a zip file\n",
    "    :param file: The zip file to extract the data from\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with ZipFile(file) as zipf:\n",
    "        # Progress Bar\n",
    "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
    "        \n",
    "        # Get features and labels from all files\n",
    "        for filename in filenames_pbar:\n",
    "            # Check if the file is a directory\n",
    "            if not filename.endswith('/'):\n",
    "                with zipf.open(filename) as image_file:\n",
    "                    image = Image.open(image_file)\n",
    "                    image.load()\n",
    "                    # Load image data as 1 dimensional array\n",
    "                    # We're using float32 to save on memory space\n",
    "                    feature = np.array(image, dtype=np.float32).flatten()\n",
    "\n",
    "                # Get the the letter from the filename.  This is the letter of the image.\n",
    "                label = os.path.split(filename)[1][0]\n",
    "\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Get the features and labels from the zip files\n",
    "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
    "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
    "\n",
    "# Limit the amount of data to work with a docker container\n",
    "docker_size_limit = 150000\n",
    "train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)\n",
    "\n",
    "# Set flags for feature engineering.  This will prevent you from skipping an important step.\n",
    "is_features_normal = False\n",
    "is_labels_encod = False\n",
    "\n",
    "# Wait until you see that all features and labels have been uncompressed.\n",
    "print('All features and labels uncompressed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Min-Max scaling for grayscale image data\n",
    "    \"\"\"\n",
    "    Min, Max = [0;255]   --> [0.1;0.9]\n",
    "    [x*Min/(max - min) + b = 0.1 ; x*Max/(max - min) + b =0.9]\n",
    "    --> b=0.1, x = 0.9-b --> x = 0.8\n",
    "    newvalue = x*value/255 + 0.1 = 0.8*value/255 + 0.1\n",
    "    \"\"\"\n",
    "    return np.add(np.multiply(0.8/255,image_data),0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data for easy access\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open('notMNIST.pickle', 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': test_features,\n",
    "                    'test_labels': test_labels,\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Data cached in pickle file.')\n",
    "\n",
    "# Reload the data\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  train_features = pickle_data['train_dataset']\n",
    "  train_labels = pickle_data['train_labels']\n",
    "  valid_features = pickle_data['valid_dataset']\n",
    "  valid_labels = pickle_data['valid_labels']\n",
    "  test_features = pickle_data['test_dataset']\n",
    "  test_labels = pickle_data['test_labels']\n",
    "  del pickle_data  # Free up memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pad images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad images with 0s\n",
    "X_train      = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_validation = np.pad(X_validation, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_test       = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "print(\"Updated Image Shape: {}\".format(X_train[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(10))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMeanAndVarianceDataSet(dataset,name):\n",
    "    print()\n",
    "    print(f'Few statistics about dataset {name} :\\n###############################')\n",
    "    print(f'- Values average/mean : {np.mean(dataset):.2f}')\n",
    "    print(f'- Values average standard deviation : {np.mean(np.std(dataset,axis=0)):.2f}')\n",
    "    print(f'- Mininum value : {np.amin(dataset):.2f}')\n",
    "    print(f'- Maximum value : {np.amax(dataset):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract table to match label indexes with German sign names :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_signIndexNames() :\n",
    "    # Read file 'signnames.csv', store in dictionary to map label indexes with sign titles\n",
    "    # Format : ClassId,SignName\n",
    "    sign_name_file = './signnames.csv'\n",
    "    with open(sign_name_file, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        table = []\n",
    "        for row in reader:\n",
    "            #print(row['ClassId'], row['SignName'])\n",
    "            table.append(row['SignName'])\n",
    "    return table\n",
    "        \n",
    "sign_table = read_signIndexNames()\n",
    "print(f'Exemple of label for image[0] : \\\"{sign_table[0]}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of image labels y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# histogram on y_train\n",
    "# Nb labels : n_classes\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "# the histogram of the data\n",
    "n, bins, patches = ax.hist(y_train,bins=n_classes,rwidth=0.8,align='left',\n",
    "                    range=(0,n_classes))\n",
    "\n",
    "ax.set_xlabel('Sign indexex')\n",
    "ax.set_ylabel('nb of sign labels')\n",
    "ax.set_title('Label Histogram')\n",
    "ax.axes.grid(b=True,which='major',axis='y')\n",
    "#ax.set_xticks(list(range(0,n_classes)), sign_table)\n",
    "plt.xticks(list(range(0,n_classes)), sign_table,rotation=90)\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "#fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayImagesSideBySide(img_list) :\n",
    "    nb_img = len(img_list)\n",
    "    f, ax = plt.subplots(nrows=1, ncols=nb_img, figsize=(9, 9))\n",
    "    f.tight_layout()\n",
    "    for i in range(nb_img) :\n",
    "        # choose cmap option if 3 color channels or if 1 (grayscale)\n",
    "        img_cmap = 'gray' if (img_list[i].shape[2] == 1) else None \n",
    "        ax[i].imshow(img_list[i].squeeze(),img_cmap)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    \n",
    "def convertToGrayScale(images):\n",
    "    images_gray = np.zeros(images.shape[:-1])\n",
    "    for i in range(images.shape[0]): \n",
    "        images_gray[i] = cv2.cvtColor(images[i], cv2.COLOR_RGB2GRAY) \n",
    "    # add channel dimension at the end\n",
    "    images_gray = np.expand_dims(images_gray, axis=3)\n",
    "    return images_gray\n",
    "\n",
    "displayImagesSideBySide(img_list=[original_img,X_train[index_random]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization for data 0 mean and equal variant¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n",
    "### converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "\n",
    "def normalize(image_data): \n",
    "    # Shape (32, 32, 3) or (32,32) if converted to gray scale.\n",
    "    return (np.divide(np.subtract(np.float32(image_data),np.float32(128)),128))\n",
    "\n",
    "print(f'len(X_train) = {len(X_train)} len(y_train) = {len(y_train)} len(X_valid) = {len(X_valid)} len(y_valid) = {len(y_valid)} len(X_test) = {len(X_test)} len(y_test) = {len(y_test)}')\n",
    "X_train = normalize(X_train)\n",
    "X_valid = normalize(X_valid)\n",
    "X_test = normalize(X_test)\n",
    "print('info debug : ')\n",
    "print(f'np.amax(X_train) = {np.amax(X_train)}') #[0][0])\n",
    "print(f'np.amin(X_train) = {np.amin(X_train)}') #[0][0])\n",
    "print(f'len(X_train) = {len(X_train)} len(y_train) = {len(y_train)} len(X_valid) = {len(X_valid)} len(y_valid) = {len(y_valid)} len(X_test) = {len(X_test)} len(y_test) = {len(y_test)}')\n",
    "\n",
    "printMeanAndVarianceDataSet(X_train,'X_train after pre-processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting X_train pixel values after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_values(X_train,400000)\n",
    "# Try to plot side by side X_train before and after normalization\n",
    "\n",
    "def plot_values(dataset,nb=1):\n",
    "    # N = 10000\n",
    "    # dataset = X_train.flatten()\n",
    "    dataset = dataset.flatten()\n",
    "    N = min(nb,len(dataset))\n",
    "    dataset = dataset[:N]\n",
    "    x = list(range(0,N))\n",
    "    plt.scatter(x, dataset, s=1)\n",
    "    plt.show()\n",
    "    \n",
    "def plotDataSetBeforeAfter(data1,data2,nb=1) :\n",
    "    data1 = data1.flatten()\n",
    "    data2 = data2.flatten()\n",
    "    N = min(nb,len(data1),len(data2))\n",
    "    data1 = data1[:N]\n",
    "    data2 = data2[:N]\n",
    "    x = list(range(0,N))\n",
    "    \n",
    "    f, ax = plt.subplots(nrows=1, ncols=2,figsize=(10, 3))  # figsize=(9, 9)\n",
    "    #f.tight_layout()\n",
    "    ax[0].scatter(x, data1, s=1)\n",
    "    ax[1].scatter(x, data2, s=1)\n",
    "    ax[0].set_title('X_train pix values before normalization')\n",
    "    ax[1].set_title('X_train pix values after normalization')\n",
    "    #ax[0].show()\n",
    "    #ax[1].show()\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "plotDataSetBeforeAfter(X_train_original,X_train,nb=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet Traffic Sign Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x, in_channels):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "        \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, in_channels, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # Test DROPOUT to prevent overfitting.\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    \n",
    "    # Test DROPOUT to prevent overfitting.\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    # Test DROPOUT to prevent overfitting.\n",
    "    logits = tf.nn.dropout(logits, keep_prob)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Output the Images\n",
    "- Problem : images (.png) were RGBA (32x32x4) instead of RGB (32x32x3).\n",
    "- So I had to use PIL.image to read .png in RGBA and convert to RGB(32x32x3), using `convert('RGB')`, to make sure that images are in the format accepted by the LeNet pipeline above (32x32x3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "from os.path import isfile\n",
    "\n",
    "import PIL.Image\n",
    "    \n",
    "web_image_folder = './webImages'\n",
    "\n",
    "filenames = os.listdir(web_image_folder)\n",
    "image_list = []\n",
    "for fname in filenames : \n",
    "    if isfile(web_image_folder + '/' + fname) :\n",
    "        image_list.append(np.array(\n",
    "            PIL.Image.open(web_image_folder + '/' + fname).convert('RGB')))\n",
    "displayImagesSideBySide(image_list)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
